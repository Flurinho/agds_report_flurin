---
title: "re_ml_01"
author: "Flurin"
date: "2023-05-02"
output: html_document
  toc: TRUE
  toc_float: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Supervised Machine Learning 1
## Comparison of linear regression and KNN models

### fitting and evaluating the models

```{r}
#load necesarry packages
library(ggplot2)
library(dbplyr)
library(tidyr)
library(caret)
library(recipes)

#import data
daily_fluxes <- readr::read_csv("./FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv")
# Data cleaning: looks ok, no obviously bad data
# no long tail, therefore no further target engineering
#create Histogram
daily_fluxes |> 
  ggplot(aes(x = GPP_NT_VUT_REF, y = ..count..)) + 
  geom_histogram()
```

```{r}


# Data splitting
set.seed(13)  # for reproducibility
split <- rsample::initial_split(daily_fluxes, prop = 0.7, strata = "VPD_F")
daily_fluxes_train <- rsample::training(split)
daily_fluxes_test <- rsample::testing(split)

# Model and pre-processing formulation, use all variables but LW_IN_F
pp <- recipes::recipe(GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, 
                      data = daily_fluxes_train |> drop_na()) |> 
  recipes::step_BoxCox(all_predictors()) |> 
  recipes::step_center(all_numeric(), -all_outcomes()) |>
  recipes::step_scale(all_numeric(), -all_outcomes())

# Fit linear regression model
mod_lm <- caret::train(
  pp, 
  data = daily_fluxes_train |> drop_na(), 
  method = "lm",
  trControl = caret::trainControl(method = "none"),
  metric = "RMSE"
)

# Fit KNN model
mod_knn <- caret::train(
  pp, 
  data = daily_fluxes_train |> drop_na(), 
  method = "knn",
  trControl = caret::trainControl(method = "none"),
  tuneGrid = data.frame(k = 8),
  metric = "RMSE"
)
```

```{r}
# make model evaluation into a function to reuse code
eval_model <- function(mod, df_train, df_test){
  
  # add predictions to the data frames
  df_train <- df_train |> 
    drop_na()
  df_train$fitted <- predict(mod, newdata = df_train)
  
  df_test <- df_test |> 
    drop_na()
  df_test$fitted <- predict(mod, newdata = df_test)
  
  # get metrics tables
  metrics_train <- df_train |> 
    yardstick::metrics(GPP_NT_VUT_REF, fitted)
  
  metrics_test <- df_test |> 
    yardstick::metrics(GPP_NT_VUT_REF, fitted)
  
  # extract values from metrics tables
  rmse_train <- metrics_train |> 
    filter(.metric == "rmse") |> 
    pull(.estimate)
  rsq_train <- metrics_train |> 
    filter(.metric == "rsq") |> 
    pull(.estimate)
  
  rmse_test <- metrics_test |> 
    filter(.metric == "rmse") |> 
    pull(.estimate)
  rsq_test <- metrics_test |> 
    filter(.metric == "rsq") |> 
    pull(.estimate)
  
  # visualise as a scatterplot
  # adding information of metrics as sub-titles
  plot_1 <- ggplot(data = df_train, aes(GPP_NT_VUT_REF, fitted)) +
    geom_point(alpha = 0.3) +
    geom_smooth(method = "lm", se = FALSE, color = "red") +
    geom_abline(slope = 1, intercept = 0, linetype = "dotted") +
    labs(subtitle = bquote( italic(R)^2 == .(format(rsq_train, digits = 2)) ~~
                            RMSE == .(format(rmse_train, digits = 3))),
         title = "Training set") +
    theme_classic()
  
  plot_2 <- ggplot(data = df_test, aes(GPP_NT_VUT_REF, fitted)) +
    geom_point(alpha = 0.3) +
    geom_smooth(method = "lm", se = FALSE, color = "red") +
    geom_abline(slope = 1, intercept = 0, linetype = "dotted") +
    labs(subtitle = bquote( italic(R)^2 == .(format(rsq_test, digits = 2)) ~~
                            RMSE == .(format(rmse_test, digits = 3))),
         title = "Test set") +
    theme_classic()
  
  out <- cowplot::plot_grid(plot_1, plot_2)
  
  return(out)
}

# linear regression model
eval_model(mod = mod_lm, df_train = daily_fluxes_train, df_test = daily_fluxes_test)
```

```{r}
# KNN
eval_model(mod = mod_knn, df_train = daily_fluxes_train, df_test = daily_fluxes_test)
```

### Interpretation of differences in the context of the bias-variance trade-off \
#### Difference between the evaluation on the training and the test

### Visualisation of temporal variations

## The role of k
For KNN, k is the (only) hyperparameter. It specifies the number of neighbours to consider for determining distances. There is always an optimum  
k. Obviously, if  k=n , we consider all observations as neighbours and each prediction is simply the meanof all observed target values  
Y , irrespective of the predictor values. This cannot be optimal and such a model is likely underfit. On the other extreme, with  k = 1 , the model will be strongly affected by the noise in the single nearest neighbour and its generalisability will suffer. This should be reflected in a poor performance on the validation data."


```{r}
# Data splitting
set.seed(13)  # for reproducibility
split <- rsample::initial_split(daily_fluxes, prop = 0.7, strata = "VPD_F")
daily_fluxes_train <- rsample::training(split)
daily_fluxes_test <- rsample::testing(split)

pp <- recipes::recipe(GPP_NT_VUT_REF ~ SW_IN_F + VPD_F, 
                      data = daily_fluxes_train |> tidyr::drop_na()) |> 
  recipes::step_BoxCox(all_predictors()) |> 
  recipes::step_center(all_numeric(), -all_outcomes()) |>
  recipes::step_scale(all_numeric(), -all_outcomes())

# Fit linear regression model
mod_lm <- caret::train(
  pp, 
  data = daily_fluxes_train |> drop_na(), 
  method = "lm",
  trControl = caret::trainControl(method = "none"),
  metric = "RMSE"
)

# Fit KNN model for different values of k
k_range <- c(1, seq(10, nrow(daily_fluxes_train), by = 50))
knn_results <- data.frame(k = numeric(length(k_range)), RMSE = numeric(length(k_range)))
for (i in seq_along(k_range)) {
  knn_k <- k_range[i]
  mod_knn <- caret::train(
    pp, 
    data = daily_fluxes_train |> drop_na(), 
    method = "knn",
    trControl = caret::trainControl(method = "cv", number = 5),
    tuneGrid = data.frame(k = knn_k),
    metric = "RMSE"
  )
  knn_pred <- predict(mod_knn, daily_fluxes_test)
  knn_actual <- daily_fluxes_test$GPP_NT_VUT_REF
  knn_rmse <- caret::RMSE(knn_pred, knn_actual)
  knn_results[i, ] <- c(knn_k, knn_rmse)
}

# Visualize the results
library(ggplot2)
results_plot <- ggplot(knn_results, aes(x = k, y = RMSE)) + 
  geom_point(size = 2) + 
  scale_x_log10() + 
  scale_y_log10() + 
  labs(x = "k", y = "RMSE", title = "KNN Model Performance") + 
  theme_bw()
print(results_plot)

```
